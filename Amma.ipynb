{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IQ6F4XNFcpZJWH9ZGIIa_qhNs2XsFPzV",
      "authorship_tag": "ABX9TyOsMBS185isBhRHh/FENdA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manish-k723/Amazon-ML-Challenge/blob/main/Amma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_F3RG0H7wCPi"
      },
      "outputs": [],
      "source": [
        " import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# from sklearn.ensemble import AdaBoostClassifier\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "# from sklearn.ensemble import BaggingClassifier\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "Z_tuK2Ff2Url"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_frac = 0.02"
      ],
      "metadata": {
        "id": "byO7k3Lm2ZTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_row(row_idx):\n",
        "    if row_idx == 0:\n",
        "        return False\n",
        "    return random.random() > sample_frac"
      ],
      "metadata": {
        "id": "1lLVlMPV2KRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv',skiprows = skip_row, usecols = [\"TITLE\", \"BULLET_POINTS\", \"DESCRIPTION\", \n",
        "                                                                                                 \"PRODUCT_TYPE_ID\",\"PRODUCT_LENGTH\"],\n",
        "                  dtype  = {'BULLET_POINTS': 'category','DESCRIPTION':'category','DESCRIPTION':'category',\"PRODUCT_TYPE_ID\": 'int16',\"PRODUCT_LENGTH\":'float32'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wegwh78awMNC",
        "outputId": "ddb4a985-0624-4b50-97ac-98f801852f03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 13.6 s, sys: 1.31 s, total: 14.9 s\n",
            "Wall time: 19.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df.sample(n=50000, random_state=42)"
      ],
      "metadata": {
        "id": "lRzmPyMJEBK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv', usecols = [\"TITLE\", \"BULLET_POINTS\", \"DESCRIPTION\", \"PRODUCT_TYPE_ID\"],\n",
        "#                       dtype  = {'BULLET_POINTS': 'category','DESCRIPTION':'category','DESCRIPTION':'category',\"PRODUCT_TYPE_ID\": 'int16'})"
      ],
      "metadata": {
        "id": "KE2BkPJTwS2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orh-berM6uph",
        "outputId": "ec80d974-447a-4e95-ba0a-8ebfa15874f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkNwAaDy6loz",
        "outputId": "255f0c3f-9b5e-484b-9024-c68f3739d0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(44804, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "      tokens = word_tokenize(text.lower())\n",
        "      tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "      tokens = [stemmer.stem(token) for token in tokens if not token in stopwords]\n",
        "      return tokens"
      ],
      "metadata": {
        "id": "a0iGLkI37Qrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "fiRLaJH4eLOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature(df,X_test):\n",
        "    # Select relevant features\n",
        "    X = df[[\"TITLE\", \"BULLET_POINTS\", \"DESCRIPTION\", \"PRODUCT_TYPE_ID\"]]\n",
        "    y = df[\"PRODUCT_LENGTH\"]\n",
        "    # X_test = test_df[[\"TITLE\", \"BULLET_POINTS\", \"DESCRIPTION\", \"PRODUCT_TYPE_ID\"]]\n",
        "    # Split the data into training and test sets\n",
        "    # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    scaler = StandardScaler()\n",
        "    # Vectorize the text data using TF-IDF\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", tokenizer=tokenize)\n",
        "    X_title = vectorizer.fit_transform(X[\"TITLE\"])\n",
        "    X_bullet = vectorizer.fit_transform(X[\"BULLET_POINTS\"])\n",
        "    X_desc = vectorizer.fit_transform(X[\"DESCRIPTION\"])\n",
        "    X = np.hstack([X_title.toarray(), X_bullet.toarray(), X_desc.toarray(), X[[\"PRODUCT_TYPE_ID\"]]])\n",
        "    # Standardize the numerical data\n",
        "    X_num = scaler.fit_transform(X[:, -1:])\n",
        "    X = np.hstack([X[:, :-1], X_num])\n",
        "    X.to_csv('X.csv')\n",
        "    pickle.dump(X,open('training.pkl','wb'))\n",
        "    \n",
        "    # X_val_title = vectorizer.transform(X_val[\"TITLE\"])\n",
        "    # X_val_bullet = vectorizer.transform(X_val[\"BULLET_POINTS\"])\n",
        "    # X_val_desc = vectorizer.transform(X_val[\"DESCRIPTION\"])\n",
        "    X_test_title = vectorizer.transform(X_test[\"TITLE\"])\n",
        "    X_test_bullet = vectorizer.transform(X_test[\"BULLET_POINTS\"])\n",
        "    X_test_desc = vectorizer.transform(X_test[\"DESCRIPTION\"])\n",
        "\n",
        "    # X_val = np.hstack([X_val_title.toarray(), X_val_bullet.toarray(), X_val_desc.toarray(), X_val[[\"PRODUCT_TYPE_ID\"]]])\n",
        "    X_test = np.hstack([X_test_title.toarray(), X_test_bullet.toarray(), X_test_desc.toarray(), X_test[[\"PRODUCT_TYPE_ID\"]]])\n",
        "\n",
        "    # X_val_num = scaler.transform(X_val[:,-1:])\n",
        "    X_test_num = scaler.transform(X_test[:, -1:])\n",
        "    # X_val = np.hstack([X_val[:, :-1], X_val_num])\n",
        "    X_test = np.hstack([X_test[:, :-1], X_test_num])\n",
        "    X_test.to_csv('X_test.csv')\n",
        "    pickle.dump(X_test,open('test.pkl','wb'))\n",
        "\n",
        "    return X, X_test, y"
      ],
      "metadata": {
        "id": "NCtx1s8CwIs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['BULLET_POINTS'] = df['BULLET_POINTS'].cat.add_categories([''])\n",
        "df['DESCRIPTION'] = df['DESCRIPTION'].cat.add_categories([''])"
      ],
      "metadata": {
        "id": "xMrEVLSj8YI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset = ['TITLE'], inplace = True)\n",
        "df['BULLET_POINTS'] = df['BULLET_POINTS'].fillna('')\n",
        "df['DESCRIPTION'] = df['DESCRIPTION'].fillna('')"
      ],
      "metadata": {
        "id": "bvt6un7awhso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_df['BULLET_POINTS'] = test_df['BULLET_POINTS'].cat.add_categories([''])\n",
        "# test_df['DESCRIPTION'] = test_df['DESCRIPTION'].cat.add_categories([''])"
      ],
      "metadata": {
        "id": "-wE-73pj85Uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_df['TITLE'] = test_df['TITLE'].fillna('')\n",
        "# test_df['BULLET_POINTS'] = test_df['BULLET_POINTS'].fillna('')\n",
        "# test_df['DESCRIPTION'] = test_df['DESCRIPTION'].fillna('')"
      ],
      "metadata": {
        "id": "ondq6azBwlDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[[\"TITLE\", \"BULLET_POINTS\", \"DESCRIPTION\", \"PRODUCT_TYPE_ID\"]]\n",
        "y = df[\"PRODUCT_LENGTH\"]\n",
        "    # X_test = test_df[[\"TITLE\", \"BULLET_POINTS\", \"DESCRIPTION\", \"PRODUCT_TYPE_ID\"]]\n",
        "    # Split the data into training and test sets\n",
        "    # X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "    # Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", tokenizer=tokenize)\n",
        "\n",
        "# X_title = vectorizer.fit_transform(X[\"TITLE\"])\n",
        "# X_bullet = vectorizer.fit_transform(X[\"BULLET_POINTS\"])\n",
        "# X_desc = vectorizer.fit_transform(X[\"DESCRIPTION\"])\n",
        "# X = np.hstack([X_title.toarray(), X_bullet.toarray(), X_desc.toarray(), X[[\"PRODUCT_TYPE_ID\"]]])\n",
        "#     # Standardize the numerical data\n",
        "# X_num = scaler.fit_transform(X[:, -1:])\n",
        "# X = np.hstack([X[:, :-1], X_num])\n",
        "# X.to_csv('X.csv')\n",
        "# pickle.dump(X,open('training.pkl','wb'))"
      ],
      "metadata": {
        "id": "kli4-CI7waac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X, X_test, y, y_test = train_test_split(X,y,test_size = 0.2,random_state=42)"
      ],
      "metadata": {
        "id": "t34WQIje3UBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = test_df.copy()"
      ],
      "metadata": {
        "id": "V865ZOj8xB5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_title = vectorizer.fit_transform(X[\"TITLE\"])\n",
        "X_test_title = vectorizer.transform(X_test[\"TITLE\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WXzEjYVw3Gw",
        "outputId": "ea9bb4f3-7e55-49fd-e095-02ccabcd2104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'le', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'u', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 25.1 s, sys: 151 ms, total: 25.3 s\n",
            "Wall time: 28.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_bullet = vectorizer.fit_transform(X[\"BULLET_POINTS\"])\n",
        "X_test_bullet = vectorizer.transform(X_test[\"BULLET_POINTS\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfV6opJSw9jN",
        "outputId": "ca0c1510-3784-4e6e-92e3-bf8c6ccc7b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 2s, sys: 211 ms, total: 1min 2s\n",
            "Wall time: 1min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle.dump(X_title, open('X_title.pkl','wb'))\n",
        "# pickle.dump(X_test_title, open('X_test_title.pkl','wb'))\n",
        "# pickle.dump(X_bullet, open('X_bullet.pkl','wb'))\n",
        "# pickle.dump(X_test_bullet, open('X_test_bullet.pkl','wb'))\n",
        "# X_title.to_csv('X_title.csv')\n",
        "# X_test_title.to_csv('X_test_title.csv')\n",
        "# X_bullet.to_csv('X_bullet.csv')\n",
        "# X_test_bullet.to_csv('X_test_bullet.csv')"
      ],
      "metadata": {
        "id": "vdwLhFgXFV4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_desc = vectorizer.fit_transform(X[\"DESCRIPTION\"])\n",
        "X_test_desc = vectorizer.transform(X_test[\"DESCRIPTION\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXU6hhX_8lQp",
        "outputId": "717cc4ba-4577-4a88-97be-4325d4c9c1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1min 3s, sys: 220 ms, total: 1min 3s\n",
            "Wall time: 1min 8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack"
      ],
      "metadata": {
        "id": "yTlvO5F78zkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle.dump(X_desc, open('X_desc.pkl','wb'))\n",
        "# pickle.dump(X_test_desc, open('X_test_desc.pkl','wb'))"
      ],
      "metadata": {
        "id": "Za7OqAJAFtvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "X = hstack([X_title.toarray(), X_bullet.toarray(), X_desc.toarray(), X[[\"PRODUCT_TYPE_ID\"]]])\n",
        "X_test = hstack([X_test_title.toarray(), X_test_bullet.toarray(), X_test_desc.toarray(), X_test[[\"PRODUCT_TYPE_ID\"]]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "GbZEjt6C8mp7",
        "outputId": "300922f0-7b83-4b72-ff90-f8f5a71f707c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-49f3dffd32fd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %%time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_bullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PRODUCT_TYPE_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_bullet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PRODUCT_TYPE_ID\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_construct.py\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \"\"\"\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/scipy/sparse/_construct.py\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \"\"\"\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m     \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (35843,59937) into shape (35843,)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_num = scaler.fit_transform(X[:, -1:])\n",
        "X = hstack([X[:, :-1], X_num])\n",
        "# X.to_csv('X.csv')\n",
        "# pickle.dump(X,open('training.pkl','wb'))"
      ],
      "metadata": {
        "id": "hC3gWCdz8oRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_test_num = scaler.transform(X_test[:, -1:])\n",
        "    # X_val = np.hstack([X_val[:, :-1], X_val_num])\n",
        "X_test = hstack([X_test[:, :-1], X_test_num])\n",
        "X_test.to_csv('X_test.csv')\n",
        "pickle.dump(X_test,open('test.pkl','wb'))\n",
        "\n",
        "    # return X, X_test, y"
      ],
      "metadata": {
        "id": "uXfPoYIh8qiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# X_train, test_df, y_train = feature(df, test_df)"
      ],
      "metadata": {
        "id": "bZ3B6RYfwlcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "jVdtRjXtfGVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LinearRegression()\n",
        "ridge = Ridge(alpha = 1.0)\n",
        "rfc = RandomForestClassifier(n_estimators=200, random_state=2)\n",
        "gbr = GradientBoostingRegressor(n_estimators=200, random_state=42)\n",
        "# bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
        "xgb = XGBRegressor(n_estimators=100,random_state=42)\n",
        "# abc = AdaBoostClassifier(n_estimators=150, random_state=2)\n",
        "svr = SVR(kernel=\"linear\")"
      ],
      "metadata": {
        "id": "PBAt2koYxCl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clfs = {\n",
        "    'svr': svr,\n",
        "    'lr' : lr,\n",
        "    'rr' : ridge,\n",
        "    'rfc': rfc,\n",
        "    'gbr': gbr,\n",
        "    'xgb': xgb\n",
        "}"
      ],
      "metadata": {
        "id": "S6LebfV2yaDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(clf,X_train,y_train):\n",
        "    clf.fit(X_train,y_train)\n",
        "    y_pred = clf.predict(X_train)\n",
        "    accuracy = mean_absolute_percentage_error(y_train,y_pred)\n",
        "    \n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "KK51Vl1TycoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "for name,clf in clfs.items():\n",
        "    \n",
        "    current_accuracy,current_precision = train_classifier(clf, X_train,)\n",
        "    \n",
        "    print(\"For \",name)\n",
        "    print(\"Accuracy - \",current_accuracy)\n",
        "    print(\"Precision - \",current_precision)\n",
        "    \n",
        "    scores.append(current_accuracy)"
      ],
      "metadata": {
        "id": "4ipW1otxye5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}